# 타이타닉 두번째 캐글커널을 깃으로 올리는 방법을 찾아보거나 로컬 주피터노트북에서 작성해서 업로드 하는 방법을 지향해야겠다

# %% [markdown]
# 

# %% [markdown]
# ## 1. Introduction

# %% [code]


# %% [markdown]
# 1. Feature analysis
# 2. Feature engineering
# 3. FeatureModeling
# 

# %% [code]


# %% [code]
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

from collections import Counter

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold,learning_curve

sns.set(style='white',context='notebook',palette='deep')

# %% [markdown]
# #  2. Load and check data

# %% [markdown]
# ### 2.1 Load data

# %% [code]
pwd

# %% [code]
train = pd.read_csv("../input/titanic/train.csv")
test = pd.read_csv('../input/titanic/test.csv')
IDtest = test['PassengerId']

# %% [code]


# %% [code]


# %% [markdown]
# ### Outlier detection

# %% [code]
def detect_outliers(df,n,features):
    outlier_indices =[]
    for col in features:
        Q1 = np.percentile(df[col],25)
        Q3 = np.percentile(df[col],75)
        IQR = Q3 - Q1
        
        outlier_step = 1.5 * IQR
        
        outlier_list_col = df[(df[col]<Q1 - outlier_step) | (df[col]>Q3 + outlier_step)].index
        
        outlier_indices.extend(outlier_list_col)
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(k for k,v in outlier_indices.items() if v>n)
    return multiple_outliers
Outliers_to_drop = detect_outliers(train,2,['Age','SibSp','Parch','Fare'])

# %% [code]
train.loc[Outliers_to_drop]

# %% [code]
train = train.drop(Outliers_to_drop,axis=0).reset_index(drop=True)

# %% [markdown]
# ### 2.3 Joining train and test set

# %% [code]
train_len = len(train)
dataset = pd.concat(objs=[train,test],axis=0).reset_index(drop=True)

# %% [raw]
# 

# %% [markdown]
# ### 2.4 check for null and missing values

# %% [code]
dataset = dataset.fillna(np.nan)

dataset.isnull().sum()

# %% [code]
train.info()
train.isnull().sum()

# %% [code]
train.head()

# %% [code]
train.describe()

# %% [code]


# %% [markdown]
# ## Feature analysis

# %% [code]
g = sns.heatmap(train[['Survived','SibSp','Parch','Age','Fare']].corr(),annot=True,fmt='.2f',cmap='coolwarm')

# %% [markdown]
# ### SibSp

# %% [code]
g = sns.factorplot(x='SibSp',y='Survived',data=train,kind='bar',size=6,palette='muted')
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [code]
g = sns.factorplot(x='Parch',y='Survived',palette='muted',kind='bar',size=6,data=train)
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [markdown]
# 3 의 지표에는 중요한 표준편차가 있다.

# %% [markdown]
# ### Age

# %% [code]
g = sns.FacetGrid(train, col='Survived')
g = g.map(sns.distplot, "Age")

# %% [code]
# Explore Age distribution
g = sns.kdeplot(train['Age'][(train['Survived']==0) & (train['Age'].notnull())],color='Red',shade=True)
g = sns.kdeplot(train['Age'][(train['Survived']==1) & (train['Age'].notnull())],ax =g,color='Blue',shade=True)
g.set_xlabel('Age')
g.set_ylabel('Frequency')
g = g.legend(['Not Survived','Survived'])

# %% [code]
dataset['Fare'].isnull().sum()

# %% [code]
dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())

# %% [code]
g = sns.distplot(dataset['Fare'],color='m',label='Skewness : %.2f'%(dataset['Fare'].skew()))
g = g.legend(loc='best')

# %% [code]
dataset['Fare'] = dataset['Fare'].map(lambda i:np.log(i) if i>0 else 0)

# %% [code]
g = sns.distplot(dataset['Fare'],color='b',label='Skewness : %.2f'%(dataset['Fare'].skew()))
g = g.legend(loc='best')

# %% [markdown]
# ## Categorical values
# 
# ### Sex

# %% [code]
g = sns.barplot(x='Sex',y='Survived',data=train)
g = g.set_ylabel('Survival Probability')

# %% [code]
train[['Sex','Survived']].groupby('Sex').mean()

# %% [markdown]
# ### Pclass

# %% [code]
g = sns.factorplot(x='Pclass',y='Survived',data=train,palette = 'muted',size=6,kind='bar')
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [code]
g = sns.factorplot(x='Pclass',y='Survived',hue='Sex',data=train,size=6,kind='bar',palette='muted')
g.despine(left=True)
g = g.set_ylabels('survival probalbilty')

# %% [markdown]
# ### Embarked

# %% [code]
dataset['Embarked'].isnull().sum()

# %% [code]
dataset.Embarked.value_counts()

# %% [code]
dataset['Embarked'] = dataset['Embarked'].fillna('S')

# %% [code]
g = sns.factorplot(x='Embarked',y='Survived',data=train,size=6,kind='bar',palette='muted')
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [code]
g = sns.factorplot('Pclass',col='Embarked',data=train,size=6,kind='count',palette='muted')
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [markdown]
# ## Filling missing Values

# %% [markdown]
# ### Age

# %% [code]
g = sns.factorplot(y='Age',x='Sex',data=dataset,kind='box')
g = sns.factorplot(y='Age',x='Sex',hue='Pclass',data=dataset,kind='box')
g = sns.factorplot(y='Age',x='Parch',data=dataset,kind='box')
g = sns.factorplot(y='Age',x='SibSp',data=dataset,kind='box')

# %% [code]
dataset['Sex'] = dataset['Sex'].map({'male':0,'female':1})

# %% [code]
g = sns.heatmap(dataset[['Age','Sex','SibSp','Parch','Pclass']].corr(),cmap='BrBG',annot=True)

# %% [code]
## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp
# Index of NaN age rows
index_NaN_age = list(dataset["Age"][dataset["Age"].isnull()].index)

for i in index_NaN_age :
    age_med = dataset["Age"].median()
    age_pred = dataset["Age"][((dataset["SibSp"] == dataset.iloc[i]["SibSp"]) & (dataset["Parch"] == dataset.iloc[i]["Parch"]) & (dataset["Pclass"] == dataset.iloc[i]["Pclass"]))].median()
    if not np.isnan(age_pred) :
        dataset["Age"].iloc[i] = age_pred
    else :
        dataset["Age"].iloc[i] = age_med
# SibSP , Parch ,Pclass 가 같은 값이 데이터셋에 있다면 그 인덱스의 나이값으로 대체  값이 없다면 평균 나이값으로 대체

# %% [code]
g = sns.factorplot(x='Survived', y ='Age',data=train , kind='box')
g = sns.factorplot(x='Survived', y ='Age',data = train,kind='violin')

# %% [markdown]
# ## Feature Engineering
# 
# ### Name/ Title

# %% [code]
dataset['Name'].head()

# %% [code]
# Get Title from Name

dataset_title=[i.split(",")[1].split('.')[0].strip() for i in dataset['Name']]
dataset['Title'] = pd.Series(dataset_title)
dataset['Title'].head()

# %% [code]
g = sns.countplot(x='Title',data=dataset)
g = plt.setp(g.get_xticklabels(),rotation=45)

# %% [code]
dataset['Title'] = dataset['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')
dataset['Title'] = dataset['Title'].map({'Master':0,'Miss':1,'Ms':1,'Mme':1,'Mlle':1,'Mrs':1,'Mr':2,'Rare':3})
dataset['Title'] = dataset['Title'].astype(int)

# %% [code]
g = sns.countplot(dataset['Title'])
g = g.set_xticklabels(['Master','Miss/Ms/Mme/Mlle/Mrs','Mr','Rare'])

# %% [code]
g = sns.factorplot(x='Title',y='Survived',data=dataset,kind='bar',size=6,palette='muted')
g.despine(left=True)
g = g.set_ylabels('survival probability')

# %% [code]
dataset.drop(labels = ['Name'],axis=1,inplace=True)

# %% [markdown]
# ### Family size

# %% [code]
dataset['Fsize'] = dataset['SibSp']+dataset['Parch']+1

# %% [code]
g = sns.factorplot(x='Fsize',y='Survived',data=dataset)
g = g.set_ylabels('Survival Probability')

# %% [code]
dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s==1 else 0)
dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if s==2 else 0)
dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3<=s<=4 else 0)
dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s>=5 else 0)

# %% [code]
g = sns.factorplot(x='Single',y='Survived',data=dataset,kind='bar')
g = g.set_ylabels('Survival Probability')
g = sns.factorplot(x='SmallF',y='Survived',data=dataset,kind='bar')
g = g.set_ylabels('Survival Probability')
g = sns.factorplot(x='MedF',y='Survived',data=dataset,kind='bar')
g = g.set_ylabels('Survival Probability')
g = sns.factorplot(x='LargeF',y='Survived',data=dataset,kind='bar')
g = g.set_ylabels('Survival Probability')

# %% [code]
dataset = pd.get_dummies(dataset, columns = ['Title'])
dataset = pd.get_dummies(dataset, columns = ['Embarked'],prefix='Em')

# %% [code]
dataset.head()

# %% [markdown]
# ### Cabin

# %% [code]
dataset['Cabin'].head()

# %% [code]
dataset['Cabin'].describe()

# %% [code]
dataset['Cabin'].isnull().sum()

# %% [code]
dataset['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin']])
# 첫 글자는 각 승객들의 공간? 식별코드 를 나타냄

# %% [code]
desk = dataset['Cabin'][dataset['Cabin'].notnull()]
desk = [first[0] for first in desk]
desk = list(set(desk))
desk

# %% [code]
g = sns.countplot(dataset['Cabin'],order=['A','B','C','D','E','F','G','T','X'])

# %% [code]
g = sns.factorplot(y='Survived',x='Cabin',data=dataset,kind='bar',order=['A','B','C','D','E','F','G','T','X'])
g = g.set_ylabels('Survival Probability')

# %% [code]
dataset = pd.get_dummies(dataset,columns=['Cabin'],prefix='Cabin')

# %% [markdown]
# ### Ticket

# %% [code]
dataset['Ticket'].head()

# %% [code]
Ticket = []
for i in list(dataset.Ticket):
    if not i.isdigit():
        Ticket.append(i.replace('.','').replace('/','').strip().split(' ')[0])
    else:
        Ticket.append('X')
dataset['Ticket'] = Ticket
dataset['Ticket'].head()

# %% [code]
dataset=pd.get_dummies(dataset,columns = ['Ticket'],prefix='T')

# %% [code]
dataset['Pclass'] = dataset['Pclass'].astype('category')
dataset=pd.get_dummies(dataset,columns = ['Pclass'],prefix='Pc')

# %% [code]
dataset.drop(labels = ['PassengerId'],axis=1,inplace=True)

# %% [code]
dataset.head()

# %% [markdown]
# ## Modeling

# %% [code]
train = dataset[:train_len]
test = dataset[train_len:]
test.drop(labels=['Survived'],axis=1,inplace=True)

# %% [code]
train['Survived'] = train['Survived'].astype(int)
Y_train = train['Survived']
X_train = train.drop(labels = ['Survived'],axis=1)

# %% [markdown]
# ### Simple modeling
# 
# ### Cross validate models

# %% [code]
kfold = StratifiedKFold(n_splits=10)

# %% [code]
random_state=2
classifiers = []
classifiers.append(SVC(random_state=random_state))
classifiers.append(DecisionTreeClassifier(random_state=random_state))
classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))
classifiers.append(RandomForestClassifier(random_state=random_state))
classifiers.append(ExtraTreesClassifier(random_state=random_state))
classifiers.append(GradientBoostingClassifier(random_state=random_state))
classifiers.append(MLPClassifier(random_state=random_state))
classifiers.append(KNeighborsClassifier())
classifiers.append(LogisticRegression(random_state=random_state))
classifiers.append(LinearDiscriminantAnalysis())

# %% [code]
cv_results = []
for classifier in classifiers :
    cv_results.append(cross_val_score(classifier, X_train,y=Y_train,scoring = 'accuracy',cv = kfold,n_jobs=4))
    
cv_means = []
cv_std =[]
for cv_result in cv_results:
    cv_means.append(cv_result.mean())
    cv_std.append(cv_result.std())
cv_res = pd.DataFrame({'CrossValMeans':cv_means,'CrossValerrors':cv_std,'Algorithm':['SVC','DecisionTree','AdaBoost','RandomForest','ExtraTrees','GradientBoosting','MultipleLayerPerceptron',
                                                                                    'Kneighboors','LogisticRegression','LinearDiscriminantAnalysis']})

g = sns.barplot('CrossValMeans','Algorithm',data=cv_res,palette='Set3',orient='h',**{'xerr':cv_std})
g.set_xlabel('Mean Accuracy')
g = g.set_title('Cross validation scores')

# %% [code]
# RFC  Parameters tunning
RFC = RandomForestClassifier()

rf_param_grid={'max_depth':[None],
              'max_features':[1,3,10],
              'min_samples_split':[2,3,10],
              'min_samples_leaf':[1,3,10],
              'bootstrap':[False],
              'n_estimators':[100,300],
              'criterion':['gini']}

gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)
gsRFC.fit(X_train,Y_train)
RFC_best = gsRFC.best_estimator_

# Best score
gsRFC.best_score_

# %% [code]
# Gradient boosting tunning

GBC = GradientBoostingClassifier()
gb_param_grid={'loss':['deviance'],
              'n_estimators':[100,200,300],
              'learning_rate':[0.1,0.05,0.01],
              'max_depth':[4,8],
              'min_samples_leaf':[100,150],
              'max_features':[0.3,0.1]}

gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid,cv=kfold,scoring='accuracy',n_jobs=4,verbose=1)

gsGBC.fit(X_train,Y_train)
GBC_best = gsGBC.best_estimator_

# Best score
gsGBC.best_score_

# %% [code]
def plot_learning_curve(estimator,title,X,y,ylim=None,cv=None, n_jobs=-1,train_sizes=np.linspace(.1,1.0,5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel('Traning example')
    plt.ylabel('Score')
    train_sizes,train_scores,test_scores = learning_curve( estimator, X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores,axis=1)
    train_scores_std = np.std(train_scores,axis=1)
    test_scores_mean = np.mean(test_scores,axis=1)
    test_scores_std = np.std(test_scores,axis=1)
    plt.grid()
    
    plt.fill_between(train_sizes,train_scores_mean - train_scores_std,train_scores_mean+train_scores_std,alpha=0.1,color='r')
    plt.fill_between(train_sizes,test_scores_mean - test_scores_std,test_scores_mean+test_scores_std,alpha=0.1,color='g')
    plt.plot(train_sizes,train_scores_mean,'o-',color='r',label='Training score')
    plt.plot(train_sizes,test_scores_mean, 'o-',color='g',label='Cross-validation score')
    
    plt.legend(loc='best')
    return plt
g = plot_learning_curve(gsRFC.best_estimator_,'RF mearning curves',X_train,Y_train,cv=kfold)
g = plot_learning_curve(gsGBC.best_estimator_,'GradientBoosting learning curves',X_train,Y_train,cv=kfold)


# %% [code]
nrows =2
ncols = 2
fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex="all", figsize=(15,15))

names_classifiers = [("RandomForest",RFC_best),("GradientBoosting",GBC_best)]

nclassifier = 0
for row in range(nrows):
    for col in range(ncols):
        name = names_classifiers[nclassifier][0]
        classifier = names_classifiers[nclassifier][1]
        indices = np.argsort(classifier.feature_importances_)[::-1][:40]
        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])
        g.set_xlabel("Relative importance",fontsize=12)
        g.set_ylabel("Features",fontsize=12)
        g.tick_params(labelsize=9)
        g.set_title(name + " feature importance")
        nclassifier += 1

# %% [code]
votingC = VotingClassifier(estimators = [('rfc',RFC_best),('gbc',GBC_best)],voting='soft',n_jobs=4)

votingC = votingC.fit(X_train,Y_train)

# %% [markdown]
# ### Prediction

# %% [code]
test_Survived = pd.Series(votingC.predict(test),name="Survived")
results = pd.concat([IDtest,test_Survived],axis=1)
results.to_csv('ensemble_python_voting.csv',index=False)



평균나이대는 20~40 으로 29 정도가 평균

클래스가 높을수록 나이가 많다

나이가 많은사람들이 (부모/자녀) 가 많다   3~ 

어릴수록 형제가 많다  20 ~ 5 살            3~

2, 3~4 인 의 가족관계의 생존률이 높음

(B-C-D-E-F) 0.5% 객실의 승객 구조확률이 높음  (A-G)는 0.4%  비슷

문자가있는 티켓의 문자일치여부로  같은 객실 혹은 관계여부를 살짝 유추할 수 있음


모델 분석 결과

Title_2  ( 여성 ) 호칭? 의 중요성이 압도적임

이외에는 
Sex  	(성별)
Title_1   ( 아동 )
Fare 
PC_3
Age
Cabin_X
항목들이 비슷하게 나타났고

나머지는 저조함
